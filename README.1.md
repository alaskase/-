# prz5

## Исходные данные
Датасет Security Patches Dataset (https://github.com/TQRG/security-patches-dataset) является набором данных, содержащим информацию о патчах безопасности для операционных систем Windows и Ubuntu, выпущенных в период с 2010 по 2020 годы. Этот датасет содержит в себе размеченные данные о характеристиках уязвимостей, которые были исправлены в каждом патче безопасности. Исследователи собрали данные из базы данных Common Vulnerabilities & Exposures (CVE), которая содержит информацию о уязвимостях программного обеспечения. Они также собрали данные из трех других наборов данных, чтобы создать датасет реальных патчей безопасности. Датасет включает информацию о патчах, такую как сообщения о коммитах, метаданные и изменения в коде. Исследователи собрали 8057 коммитов, связанных с безопасностью, из 1339 различных проектов на 20 языках программирования. Они также создали датасет из 110 000 коммитов, не связанных с безопасностью, для целей машинного обучения. Каждый патч безопасности включает следующую информацию:

* Идентификатор патча
* Версия операционной системы
* Дата выпуска патча
* Описание уязвимости
* Уровень важности уязвимости
* Тип уязвимости (например, DoS, RCE, XSS и т.д.)
* Информация об уязвимости (например, имя файла, содержащего уязвимый код)
* Список открытых источников, в которых была опубликована информация об уязвимости

Датасет включает в себя более 16 тысяч патчей безопасности, и он может использоваться для обучения алгоритмов машинного обучения в области обнаружения и анализа уязвимостей в операционных системах.

На основе датасета Security Patches Dataset можно решить следующие задачи:

1. Классификация уязвимостей: на основе данных о характеристиках уязвимостей можно обучить модель классификации для определения типа уязвимости (например, DoS, RCE, XSS и т.д.).

2. Обнаружение уязвимостей: можно обучить модель машинного обучения для обнаружения уязвимостей в операционных системах на основе данных об исправленных уязвимостях в патчах безопасности.

3. Анализ уязвимостей: можно провести анализ уязвимостей в операционных системах, используя данные о характеристиках уязвимостей и свойствах патчей безопасности.

4. Прогнозирование риска безопасности: на основе данных о характеристиках уязвимостей и уровнях важности патчей безопасности можно обучить модель прогнозирования риска безопасности, которая может помочь оценить уровень угрозы безопасности для операционных систем.

5. Оценка эффективности патчей безопасности: можно проанализировать данные о патчах безопасности, чтобы определить эффективность исправлений и оценить качество обновлений операционных систем.

## Классификация уязвимостей
Первая задача, которую можно решить на основе датасета Security Patches Dataset - это классификация уязвимостей.

Для решения этой задачи можно использовать алгоритмы машинного обучения, такие как метод опорных векторов (SVM), случайный лес (Random Forest), градиентный бустинг (Gradient Boosting) и другие.

Шаги для решения задачи классификации уязвимостей на основе данного датасета могут быть следующими:

1. Подготовка данных: необходимо загрузить данные из датасета, провести их предварительную обработку, выделить признаки (features) и целевую переменную (target), а также разделить данные на обучающую и тестовую выборки.

2. Выбор модели: необходимо выбрать модель машинного обучения, которая будет использоваться для решения задачи классификации уязвимостей на основе данного датасета. Можно использовать различные алгоритмы машинного обучения и подобрать оптимальные гиперпараметры.

3. Обучение модели: необходимо обучить модель машинного обучения на обучающей выборке.

4. Оценка качества модели: необходимо оценить качество модели на тестовой выборке. Можно использовать метрики качества, такие как accuracy, precision, recall, F1-score и другие.

5. Тюнинг модели: при необходимости можно провести дополнительный тюнинг модели, подбирая оптимальные гиперпараметры для улучшения качества классификации.

6. Использование модели: после обучения модели можно использовать ее для классификации новых данных, чтобы определить тип уязвимости.

Для решения задачи классификации уязвимостей можно использовать различные инструменты и библиотеки машинного обучения на языке Python, такие как scikit-learn, TensorFlow, PyTorch и другие.

## Выбор признакового пространства
Конкретный вариант выбора признаков для данного датасета может включать следующие шаги:

1. Выполнить предварительную обработку данных: удалить ненужные столбцы, заполнить пропущенные значения, преобразовать категориальные признаки в числовые.

2. Рассчитать матрицу корреляции между признаками и целевой переменной. Отобрать признаки с наивысшей абсолютной корреляцией.

3. Обучить модель машинного обучения на всем признаковом пространстве и оценить веса признаков. Отобрать признаки с наивысшими весами.

4. Применить метод главных компонент (PCA) для снижения размерности признакового пространства. Отобрать главные компоненты с наивысшей объясненной дисперсией.

5. Применить метод взаимной информации для определения важности признаков на основе их взаимной зависимости с целевой переменной.

6. Сравнить результаты различных методов выбора признаков и выбрать наиболее подходящий вариант.

Обратите внимание, что выбор признаков является итеративным процессом, который может потребовать нескольких итераций и изменения параметров выбранных методов.

## Вариант выбора признакового пространства № 1

Шаг 1: Предварительная обработка данных

```python
import pandas as pd

# Загрузка данных
data = pd.read_csv('security-patches-dataset.csv')

# Удаление ненужных столбцов
data.drop(['ID', 'CVE', 'Description'], axis=1, inplace=True)

# Заполнение пропущенных значений
data.fillna(data.mean(), inplace=True)

# Преобразование категориальных признаков в числовые
data = pd.get_dummies(data, columns=['Vendor', 'Product', 'Version', 'Severity'])
```

Шаг 2: Отбор признаков на основе корреляции

```python
import seaborn as sns

# Вычисление матрицы корреляции
corr = data.corr()

# Отображение тепловой карты
sns.heatmap(corr)

# Отбор признаков с наивысшей корреляцией
relevant_features = corr['Patched'].sort_values(ascending=False)[1:6].index.tolist()
```

Шаг 3: Отбор признаков на основе весов модели

```python
from sklearn.linear_model import LogisticRegression

# Разделение данных на признаки и целевую переменную
X = data.drop('Patched', axis=1)
y = data['Patched']

# Обучение модели
model = LogisticRegression()
model.fit(X, y)

# Отображение весов признаков
feature_weights = pd.DataFrame({'feature': X.columns.values, 'weight': model.coef_[0]})
feature_weights = feature_weights.sort_values(by='weight', ascending=False)

# Отбор признаков с наивысшими весами
relevant_features = feature_weights['feature'][:5].tolist()
```

Шаг 4: Снижение размерности признакового пространства с помощью PCA

```python
from sklearn.decomposition import PCA

# Стандартизация данных
X = (X - X.mean()) / X.std()

# Применение PCA
pca = PCA(n_components=5)
pca.fit(X)

# Отображение объясненной дисперсии
explained_variance = pd.DataFrame({'component': range(1, 6), 'explained_variance': pca.explained_variance_ratio_})
sns.barplot(x='component', y='explained_variance', data=explained_variance)

# Отбор главных компонент
relevant_features = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5']
```

Шаг 5: Отбор признаков на основе взаимной информации

```python
from sklearn.feature_selection import mutual_info_classif

# Вычисление взаимной информации
mi = mutual_info_classif(X, y)

# Отображение взаимной информации для каждого признака
mi_df = pd.DataFrame({'feature': X.columns.values, 'mi': mi})
mi_df = mi_df.sort_values(by='mi', ascending=False)

# Отбор признаков с наивысшей взаимной информацией
relevant_features = mi_df['feature'][:5].tolist()
```

После выбора признаков можно использовать их для обучения модели машинного обучения и выполнения задачи классификации уязвимостей. Вот пример кода для создания и обучения модели логистической регрессии с использованием выбранных признаков:

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X[relevant_features], y, test_size=0.2)

# Обучение модели
model = LogisticRegression()
model.fit(X_train, y_train)

# Оценка качества модели на тестовой выборке
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
```

Этот код обучает модель логистической регрессии на обучающей выборке, используя только выбранные признаки, а затем оценивает ее точность на тестовой выборке с помощью метрики accuracy. Вы можете использовать другие методы машинного обучения и метрики оценки в зависимости от вашей задачи.

## Вариант выбора признакового пространства № 2

1. Предварительная обработка данных

```python
import pandas as pd

# загрузка данных
df = pd.read_csv('security-patches-dataset.csv')

# удаление ненужных столбцов
df = df.drop(['patch_name', 'patch_description'], axis=1)

# заполнение пропущенных значений
df = df.fillna(df.mean())

# преобразование категориальных признаков в числовые
df = pd.get_dummies(df, columns=['vendor', 'product', 'severity'])
```

2. Корреляция признаков

```python
import seaborn as sns

# расчет матрицы корреляции
corr_matrix = df.corr()

# отбор признаков с наивысшей абсолютной корреляцией
highest_corr_features = corr_matrix.nlargest(10, 'is_vulnerable')['is_vulnerable'].index
```

3. Веса признаков

```python
from sklearn.ensemble import RandomForestClassifier

# разбиение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# обучение модели
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# отбор признаков с наивысшими весами
feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': rf.feature_importances_})
highest_weight_features = feature_importances.nlargest(10, 'importance')['feature'].values
```

4. Метод главных компонент

```python
from sklearn.decomposition import PCA

# создание объекта PCA и определение количества главных компонент
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X)

# отбор главных компонент с наивысшей объясненной дисперсией
highest_variance_components = pd.Series(pca.explained_variance_ratio_, index=range(10)).nlargest(5).index
```

5. Метод взаимной информации

```python
from sklearn.feature_selection import mutual_info_classif

# расчет взаимной информации
mutual_info = mutual_info_classif(X, y)

# отбор признаков с наивысшей важностью
highest_info_features = pd.Series(mutual_info, index=X.columns).nlargest(10).index
```

Обратите внимание, что это примеры кода, и реализация может отличаться в зависимости от ваших потребностей.

# Требуется
1. Выбрать датасет, предобработать и описать датасет, визуализировать его
2. Выбрать признаковое пространство
3. Выбрать модель машинного обучения, дающую наулучшую оценку точности по F1-мере, подготовить отчет по результатам и загрузить на гитлаб
